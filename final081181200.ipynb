{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3400 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n",
      "Class #0 = cats\n",
      "Class #1 = dogs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 12:15:09.348933  1900 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 272, 272, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 135, 135, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 135, 135, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 135, 135, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 133, 133, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 133, 133, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 133, 133, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 133, 133, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 133, 133, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 133, 133, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 66, 66, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 66, 66, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 66, 66, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 66, 66, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 31, 31, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 31, 31, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 31, 31, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 31, 31, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 31, 31, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 31, 31, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 31, 31, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 31, 31, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 31, 31, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 31, 31, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 31, 31, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 31, 31, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 31, 31, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 31, 31, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 31, 31, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 31, 31, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 31, 31, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 31, 31, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 31, 31, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 31, 31, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 31, 31, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 31, 31, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 31, 31, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 31, 31, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 31, 31, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 31, 31, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 31, 31, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 31, 31, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 31, 31, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 31, 31, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 31, 31, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 31, 31, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 31, 31, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 31, 31, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 31, 31, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 31, 31, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 31, 31, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 31, 31, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 31, 31, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 31, 31, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 31, 31, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 31, 31, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 31, 31, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 31, 31, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 31, 31, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 31, 31, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 31, 31, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 31, 31, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 31, 31, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 31, 31, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 31, 31, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 31, 31, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 31, 31, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 31, 31, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 31, 31, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 31, 31, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 31, 31, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 31, 31, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 31, 31, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 31, 31, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 31, 31, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 31, 31, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 31, 31, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 31, 31, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 31, 31, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 31, 31, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 31, 31, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 31, 31, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 31, 31, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 31, 31, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 31, 31, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 31, 31, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 31, 31, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 31, 31, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 31, 31, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 31, 31, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 15, 15, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 15, 15, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 15, 15, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 15, 15, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 15, 15, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 15, 15, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 15, 15, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 15, 15, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 15, 15, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 15, 15, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 15, 15, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 15, 15, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 15, 15, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 15, 15, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 15, 15, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 15, 15, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 15, 15, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 15, 15, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 15, 15, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 15, 15, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 15, 15, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 15, 15, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 15, 15, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 15, 15, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 15, 15, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 15, 15, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 15, 15, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 15, 15, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 15, 15, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 15, 15, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 15, 15, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 15, 15, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 15, 15, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 15, 15, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 15, 15, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 15, 15, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 15, 15, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 15, 15, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 15, 15, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 15, 15, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 15, 15, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 15, 15, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 15, 15, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 15, 15, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 15, 15, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 15, 15, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 15, 15, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 15, 15, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 15, 15, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 15, 15, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 15, 15, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 15, 15, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 15, 15, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 15, 15, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 15, 15, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 15, 15, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 15, 15, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 15, 15, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 15, 15, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 15, 15, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 15, 15, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 15, 15, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 15, 15, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 15, 15, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 15, 15, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 15, 15, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 15, 15, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 15, 15, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 15, 15, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 15, 15, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 15, 15, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 15, 15, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 15, 15, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 15, 15, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 15, 15, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 15, 15, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 15, 15, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 15, 15, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 15, 15, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 15, 15, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 15, 15, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 15, 15, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 15, 15, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 15, 15, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 15, 15, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 15, 15, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 15, 15, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 15, 15, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 15, 15, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 15, 15, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 15, 15, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 15, 15, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 15, 15, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 15, 15, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 15, 15, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 15, 15, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 15, 15, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 15, 15, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 15, 15, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 15, 15, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 15, 15, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 15, 15, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 15, 15, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 15, 15, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 15, 15, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 15, 15, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 15, 15, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 15, 15, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 15, 15, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 15, 15, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 15, 15, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 15, 15, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 15, 15, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 15, 15, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 15, 15, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 15, 15, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 15, 15, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 15, 15, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 15, 15, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 15, 15, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 15, 15, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 15, 15, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 15, 15, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 15, 15, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 15, 15, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 15, 15, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 15, 15, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 15, 15, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 15, 15, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 15, 15, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 15, 15, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 15, 15, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 15, 15, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 15, 15, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 15, 15, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 15, 15, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 15, 15, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 15, 15, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 15, 15, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 15, 15, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 15, 15, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 15, 15, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 15, 15, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 15, 15, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 15, 15, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 15, 15, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 15, 15, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 15, 15, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 7, 7, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 7, 7, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 7, 7, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 7, 7, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 7, 7, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 7, 7, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 7, 7, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 7, 7, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 7, 7, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 7, 7, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 7, 7, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 7, 7, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 7, 7, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 7, 7, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 7, 7, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 7, 7, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 7, 7, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 7, 7, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 7, 7, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 7, 7, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 7, 7, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 7, 7, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 7, 7, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 7, 7, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 7, 7, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 7, 7, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 7, 7, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 7, 7, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 7, 7, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 7, 7, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 7, 7, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 7, 7, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 7, 7, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 7, 7, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 7, 7, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 7, 7, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 7, 7, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 7, 7, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 7, 7, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 7, 7, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 7, 7, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 7, 7, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 7, 7, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 7, 7, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 7, 7, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 7, 7, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 7, 7, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 7, 7, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 7, 7, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 7, 7, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 7, 7, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 7, 7, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 7, 7, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 7, 7, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 7, 7, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 7, 7, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 7, 7, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 7, 7, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 7, 7, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 7, 7, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 7, 7, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 7, 7, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 7, 7, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 7, 7, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 7, 7, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 7, 7, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7, 7, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 7, 7, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 7, 7, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100352)       0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100352)       0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 2)            200706      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,003,490\n",
      "Trainable params: 21,969,058\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9130\n",
      "Epoch 00001: acc improved from -inf to 0.91265, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 120s 353ms/step - loss: 0.2747 - acc: 0.9126 - val_loss: 0.1228 - val_acc: 0.9750\n",
      "Epoch 2/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9540\n",
      "Epoch 00002: acc improved from 0.91265 to 0.95412, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 68s 201ms/step - loss: 0.1199 - acc: 0.9541 - val_loss: 0.0626 - val_acc: 0.9750\n",
      "Epoch 3/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9681\n",
      "Epoch 00003: acc improved from 0.95412 to 0.96765, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 69s 202ms/step - loss: 0.1017 - acc: 0.9676 - val_loss: 0.1159 - val_acc: 0.9683\n",
      "Epoch 4/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9590\n",
      "Epoch 00004: acc did not improve from 0.96765\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.1253 - acc: 0.9591 - val_loss: 0.0552 - val_acc: 0.9783\n",
      "Epoch 5/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9794\n",
      "Epoch 00005: acc improved from 0.96765 to 0.97941, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 69s 204ms/step - loss: 0.0594 - acc: 0.9794 - val_loss: 0.0316 - val_acc: 0.9883\n",
      "Epoch 6/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9811- ETA: 1s - loss: 0.0491 - acc\n",
      "Epoch 00006: acc improved from 0.97941 to 0.98118, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 69s 202ms/step - loss: 0.0493 - acc: 0.9812 - val_loss: 0.0497 - val_acc: 0.9817\n",
      "Epoch 7/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9799\n",
      "Epoch 00007: acc did not improve from 0.98118\n",
      "340/340 [==============================] - 68s 201ms/step - loss: 0.0612 - acc: 0.9800 - val_loss: 0.0666 - val_acc: 0.9800\n",
      "Epoch 8/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9844\n",
      "Epoch 00008: acc improved from 0.98118 to 0.98441, saving model to 08180738weights.best.hdf5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "340/340 [==============================] - 69s 204ms/step - loss: 0.0392 - acc: 0.9844 - val_loss: 0.0924 - val_acc: 0.9817\n",
      "Epoch 9/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9914\n",
      "Epoch 00009: acc improved from 0.98441 to 0.99147, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 69s 203ms/step - loss: 0.0289 - acc: 0.9915 - val_loss: 0.0573 - val_acc: 0.9850\n",
      "Epoch 10/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9932\n",
      "Epoch 00010: acc improved from 0.99147 to 0.99324, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 69s 202ms/step - loss: 0.0170 - acc: 0.9932 - val_loss: 0.0572 - val_acc: 0.9833\n",
      "Epoch 11/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9953\n",
      "Epoch 00011: acc improved from 0.99324 to 0.99529, saving model to 08180738weights.best.hdf5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "340/340 [==============================] - 69s 203ms/step - loss: 0.0138 - acc: 0.9953 - val_loss: 0.0533 - val_acc: 0.9850\n",
      "Epoch 12/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9973\n",
      "Epoch 00012: acc improved from 0.99529 to 0.99735, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 69s 203ms/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0504 - val_acc: 0.9833\n",
      "Epoch 13/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9959\n",
      "Epoch 00013: acc did not improve from 0.99735\n",
      "340/340 [==============================] - 68s 201ms/step - loss: 0.0120 - acc: 0.9959 - val_loss: 0.0533 - val_acc: 0.9850\n",
      "Epoch 14/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00014: acc improved from 0.99735 to 0.99765, saving model to 08180738weights.best.hdf5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "340/340 [==============================] - 69s 204ms/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0522 - val_acc: 0.9850\n",
      "Epoch 15/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9973\n",
      "Epoch 00015: acc did not improve from 0.99765\n",
      "340/340 [==============================] - 68s 201ms/step - loss: 0.0132 - acc: 0.9971 - val_loss: 0.0513 - val_acc: 0.9850\n",
      "Epoch 16/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9971\n",
      "Epoch 00016: acc did not improve from 0.99765\n",
      "340/340 [==============================] - 68s 201ms/step - loss: 0.0102 - acc: 0.9971 - val_loss: 0.0514 - val_acc: 0.9833\n",
      "Epoch 17/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9953\n",
      "Epoch 00017: acc did not improve from 0.99765\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "340/340 [==============================] - 69s 202ms/step - loss: 0.0191 - acc: 0.9950 - val_loss: 0.0512 - val_acc: 0.9833\n",
      "Epoch 18/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9965\n",
      "Epoch 00018: acc did not improve from 0.99765\n",
      "340/340 [==============================] - 69s 202ms/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0506 - val_acc: 0.9850\n",
      "Epoch 19/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9965\n",
      "Epoch 00019: acc did not improve from 0.99765\n",
      "340/340 [==============================] - 69s 202ms/step - loss: 0.0168 - acc: 0.9965 - val_loss: 0.0517 - val_acc: 0.9833\n",
      "Epoch 20/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9968\n",
      "Epoch 00020: acc did not improve from 0.99765\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0095 - acc: 0.9968 - val_loss: 0.0507 - val_acc: 0.9833\n",
      "Epoch 21/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00021: acc did not improve from 0.99765\n",
      "340/340 [==============================] - 71s 208ms/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0508 - val_acc: 0.9833\n",
      "Epoch 22/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9959\n",
      "Epoch 00022: acc did not improve from 0.99765\n",
      "340/340 [==============================] - 69s 203ms/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0527 - val_acc: 0.9833\n",
      "Epoch 23/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9979\n",
      "Epoch 00023: acc improved from 0.99765 to 0.99794, saving model to 08180738weights.best.hdf5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0527 - val_acc: 0.9833\n",
      "Epoch 24/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9976\n",
      "Epoch 00024: acc did not improve from 0.99794\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0507 - val_acc: 0.9850\n",
      "Epoch 25/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9944\n",
      "Epoch 00025: acc did not improve from 0.99794\n",
      "340/340 [==============================] - 67s 198ms/step - loss: 0.0208 - acc: 0.9944 - val_loss: 0.0507 - val_acc: 0.9850\n",
      "Epoch 26/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9976- ETA: 1s - loss: 0.0081 - acc:\n",
      "Epoch 00026: acc did not improve from 0.99794\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "340/340 [==============================] - 67s 197ms/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0527 - val_acc: 0.9833\n",
      "Epoch 27/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9953\n",
      "Epoch 00027: acc did not improve from 0.99794\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0518 - val_acc: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9950\n",
      "Epoch 00028: acc did not improve from 0.99794\n",
      "340/340 [==============================] - 67s 197ms/step - loss: 0.0158 - acc: 0.9950 - val_loss: 0.0538 - val_acc: 0.9850\n",
      "Epoch 29/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9965- ETA: \n",
      "Epoch 00029: acc did not improve from 0.99794\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-12.\n",
      "340/340 [==============================] - 67s 198ms/step - loss: 0.0108 - acc: 0.9962 - val_loss: 0.0520 - val_acc: 0.9833\n",
      "Epoch 30/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9988\n",
      "Epoch 00030: acc improved from 0.99794 to 0.99882, saving model to 08180738weights.best.hdf5\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.0508 - val_acc: 0.9833\n",
      "Epoch 31/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9950\n",
      "Epoch 00031: acc did not improve from 0.99882\n",
      "340/340 [==============================] - 69s 204ms/step - loss: 0.0212 - acc: 0.9950 - val_loss: 0.0493 - val_acc: 0.9867\n",
      "Epoch 32/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9962\n",
      "Epoch 00032: acc did not improve from 0.99882\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0115 - acc: 0.9962 - val_loss: 0.0516 - val_acc: 0.9833\n",
      "Epoch 33/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00033: acc did not improve from 0.99882\n",
      "340/340 [==============================] - 68s 199ms/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0520 - val_acc: 0.9833\n",
      "Epoch 34/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9944\n",
      "Epoch 00034: acc did not improve from 0.99882\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0096 - acc: 0.9944 - val_loss: 0.0528 - val_acc: 0.9833\n",
      "Epoch 35/35\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9971\n",
      "Epoch 00035: acc did not improve from 0.99882\n",
      "340/340 [==============================] - 68s 200ms/step - loss: 0.0094 - acc: 0.9971 - val_loss: 0.0507 - val_acc: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16e6dda0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "import itertools\n",
    "\n",
    "# \n",
    "DATASET_PATH  = 'data'\n",
    "\n",
    "# \n",
    "IMAGE_SIZE = (272, 272)\n",
    "\n",
    "# \n",
    "NUM_CLASSES = 2\n",
    "\n",
    "#  GPU  batch size \n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# \n",
    "FREEZE_LAYERS = 0\n",
    "\n",
    "# Epoch \n",
    "NUM_EPOCHS = 35\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#  data augmentation \n",
    "train_datagen = ImageDataGenerator(rotation_range=30,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.1,\n",
    "                                   channel_shift_range=15,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_batches = train_datagen.flow_from_directory(DATASET_PATH + '/train',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_datagen = ImageDataGenerator()\n",
    "valid_batches = valid_datagen.flow_from_directory(DATASET_PATH + '/valid',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=BATCH_SIZE)\n",
    "\n",
    "# \n",
    "for cls, idx in train_batches.class_indices.items():\n",
    "    print('Class #{} = {}'.format(idx, cls))\n",
    "\n",
    "\n",
    "\n",
    "#  ResNet50 \n",
    "#  ResNet50  fully connected layers\n",
    "net = InceptionV3(include_top=False, weights='imagenet', input_tensor=None,\n",
    "               input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
    "x = net.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "#  DropOut layer\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "#  Dense layer softmax \n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)\n",
    "\n",
    "# \n",
    "net_final = Model(inputs=net.input, outputs=output_layer)\n",
    "for layer in net_final.layers[:FREEZE_LAYERS]:\n",
    "    layer.trainable = False\n",
    "for layer in net_final.layers[FREEZE_LAYERS:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "#  Adam optimizer learning rate  fine-tuning\n",
    "net_final.compile(optimizer=Adam(lr=0.0001),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# \n",
    "print(net_final.summary())\n",
    "\n",
    "file_path=\"08180738weights.best.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early = EarlyStopping(monitor=\"acc\", mode=\"max\", patience=15)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                              min_lr=1e-12, \n",
    "                              monitor='val_loss', \n",
    "                              patience=3, \n",
    "                              verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, early,reduce_lr] #early\n",
    "\n",
    "# \n",
    "net_final.fit_generator(train_batches,\n",
    "                        steps_per_epoch = train_batches.samples // BATCH_SIZE,\n",
    "                        validation_data = valid_batches,\n",
    "                        validation_steps = valid_batches.samples // BATCH_SIZE,\n",
    "                        epochs = NUM_EPOCHS,\n",
    "                              shuffle=True, \n",
    "                              verbose=True,\n",
    "                              callbacks=callbacks_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 12:56:27.795692  1900 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 12:56:27.796693  1900 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 12:56:27.799693  1900 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\test\\\\000.jpg', 'data\\\\test\\\\001.jpg', 'data\\\\test\\\\002.jpg', 'data\\\\test\\\\003.jpg', 'data\\\\test\\\\004.jpg', 'data\\\\test\\\\005.jpg', 'data\\\\test\\\\006.jpg', 'data\\\\test\\\\007.jpg', 'data\\\\test\\\\008.jpg', 'data\\\\test\\\\009.jpg', 'data\\\\test\\\\010.jpg', 'data\\\\test\\\\011.jpg', 'data\\\\test\\\\012.jpg', 'data\\\\test\\\\013.jpg', 'data\\\\test\\\\014.jpg', 'data\\\\test\\\\015.jpg', 'data\\\\test\\\\016.jpg', 'data\\\\test\\\\017.jpg', 'data\\\\test\\\\018.jpg', 'data\\\\test\\\\019.jpg', 'data\\\\test\\\\020.jpg', 'data\\\\test\\\\021.jpg', 'data\\\\test\\\\022.jpg', 'data\\\\test\\\\023.jpg', 'data\\\\test\\\\024.jpg', 'data\\\\test\\\\025.jpg', 'data\\\\test\\\\026.jpg', 'data\\\\test\\\\027.jpg', 'data\\\\test\\\\028.jpg', 'data\\\\test\\\\029.jpg', 'data\\\\test\\\\030.jpg', 'data\\\\test\\\\031.jpg', 'data\\\\test\\\\032.jpg', 'data\\\\test\\\\033.jpg', 'data\\\\test\\\\034.jpg', 'data\\\\test\\\\035.jpg', 'data\\\\test\\\\036.jpg', 'data\\\\test\\\\037.jpg', 'data\\\\test\\\\038.jpg', 'data\\\\test\\\\039.jpg', 'data\\\\test\\\\040.jpg', 'data\\\\test\\\\041.jpg', 'data\\\\test\\\\042.jpg', 'data\\\\test\\\\043.jpg', 'data\\\\test\\\\044.jpg', 'data\\\\test\\\\045.jpg', 'data\\\\test\\\\046.jpg', 'data\\\\test\\\\047.jpg', 'data\\\\test\\\\048.jpg', 'data\\\\test\\\\049.jpg', 'data\\\\test\\\\050.jpg', 'data\\\\test\\\\051.jpg', 'data\\\\test\\\\052.jpg', 'data\\\\test\\\\053.jpg', 'data\\\\test\\\\054.jpg', 'data\\\\test\\\\055.jpg', 'data\\\\test\\\\056.jpg', 'data\\\\test\\\\057.jpg', 'data\\\\test\\\\058.jpg', 'data\\\\test\\\\059.jpg', 'data\\\\test\\\\060.jpg', 'data\\\\test\\\\061.jpg', 'data\\\\test\\\\062.jpg', 'data\\\\test\\\\063.jpg', 'data\\\\test\\\\064.jpg', 'data\\\\test\\\\065.jpg', 'data\\\\test\\\\066.jpg', 'data\\\\test\\\\067.jpg', 'data\\\\test\\\\068.jpg', 'data\\\\test\\\\069.jpg', 'data\\\\test\\\\070.jpg', 'data\\\\test\\\\071.jpg', 'data\\\\test\\\\072.jpg', 'data\\\\test\\\\073.jpg', 'data\\\\test\\\\074.jpg', 'data\\\\test\\\\075.jpg', 'data\\\\test\\\\076.jpg', 'data\\\\test\\\\077.jpg', 'data\\\\test\\\\078.jpg', 'data\\\\test\\\\079.jpg', 'data\\\\test\\\\080.jpg', 'data\\\\test\\\\081.jpg', 'data\\\\test\\\\082.jpg', 'data\\\\test\\\\083.jpg', 'data\\\\test\\\\084.jpg', 'data\\\\test\\\\085.jpg', 'data\\\\test\\\\086.jpg', 'data\\\\test\\\\087.jpg', 'data\\\\test\\\\088.jpg', 'data\\\\test\\\\089.jpg', 'data\\\\test\\\\090.jpg', 'data\\\\test\\\\091.jpg', 'data\\\\test\\\\092.jpg', 'data\\\\test\\\\093.jpg', 'data\\\\test\\\\094.jpg', 'data\\\\test\\\\095.jpg', 'data\\\\test\\\\096.jpg', 'data\\\\test\\\\097.jpg', 'data\\\\test\\\\098.jpg', 'data\\\\test\\\\099.jpg', 'data\\\\test\\\\100.jpg', 'data\\\\test\\\\101.jpg', 'data\\\\test\\\\102.jpg', 'data\\\\test\\\\103.jpg', 'data\\\\test\\\\104.jpg', 'data\\\\test\\\\105.jpg', 'data\\\\test\\\\106.jpg', 'data\\\\test\\\\107.jpg', 'data\\\\test\\\\108.jpg', 'data\\\\test\\\\109.jpg', 'data\\\\test\\\\110.jpg', 'data\\\\test\\\\111.jpg', 'data\\\\test\\\\112.jpg', 'data\\\\test\\\\113.jpg', 'data\\\\test\\\\114.jpg', 'data\\\\test\\\\115.jpg', 'data\\\\test\\\\116.jpg', 'data\\\\test\\\\117.jpg', 'data\\\\test\\\\118.jpg', 'data\\\\test\\\\119.jpg', 'data\\\\test\\\\120.jpg', 'data\\\\test\\\\121.jpg', 'data\\\\test\\\\122.jpg', 'data\\\\test\\\\123.jpg', 'data\\\\test\\\\124.jpg', 'data\\\\test\\\\125.jpg', 'data\\\\test\\\\126.jpg', 'data\\\\test\\\\127.jpg', 'data\\\\test\\\\128.jpg', 'data\\\\test\\\\129.jpg', 'data\\\\test\\\\130.jpg', 'data\\\\test\\\\131.jpg', 'data\\\\test\\\\132.jpg', 'data\\\\test\\\\133.jpg', 'data\\\\test\\\\134.jpg', 'data\\\\test\\\\135.jpg', 'data\\\\test\\\\136.jpg', 'data\\\\test\\\\137.jpg', 'data\\\\test\\\\138.jpg', 'data\\\\test\\\\139.jpg', 'data\\\\test\\\\140.jpg', 'data\\\\test\\\\141.jpg', 'data\\\\test\\\\142.jpg', 'data\\\\test\\\\143.jpg', 'data\\\\test\\\\144.jpg', 'data\\\\test\\\\145.jpg', 'data\\\\test\\\\146.jpg', 'data\\\\test\\\\147.jpg', 'data\\\\test\\\\148.jpg', 'data\\\\test\\\\149.jpg', 'data\\\\test\\\\150.jpg', 'data\\\\test\\\\151.jpg', 'data\\\\test\\\\152.jpg', 'data\\\\test\\\\153.jpg', 'data\\\\test\\\\154.jpg', 'data\\\\test\\\\155.jpg', 'data\\\\test\\\\156.jpg', 'data\\\\test\\\\157.jpg', 'data\\\\test\\\\158.jpg', 'data\\\\test\\\\159.jpg', 'data\\\\test\\\\160.jpg', 'data\\\\test\\\\161.jpg', 'data\\\\test\\\\162.jpg', 'data\\\\test\\\\163.jpg', 'data\\\\test\\\\164.jpg', 'data\\\\test\\\\165.jpg', 'data\\\\test\\\\166.jpg', 'data\\\\test\\\\167.jpg', 'data\\\\test\\\\168.jpg', 'data\\\\test\\\\169.jpg', 'data\\\\test\\\\170.jpg', 'data\\\\test\\\\171.jpg', 'data\\\\test\\\\172.jpg', 'data\\\\test\\\\173.jpg', 'data\\\\test\\\\174.jpg', 'data\\\\test\\\\175.jpg', 'data\\\\test\\\\176.jpg', 'data\\\\test\\\\177.jpg', 'data\\\\test\\\\178.jpg', 'data\\\\test\\\\179.jpg', 'data\\\\test\\\\180.jpg', 'data\\\\test\\\\181.jpg', 'data\\\\test\\\\182.jpg', 'data\\\\test\\\\183.jpg', 'data\\\\test\\\\184.jpg', 'data\\\\test\\\\185.jpg', 'data\\\\test\\\\186.jpg', 'data\\\\test\\\\187.jpg', 'data\\\\test\\\\188.jpg', 'data\\\\test\\\\189.jpg', 'data\\\\test\\\\190.jpg', 'data\\\\test\\\\191.jpg', 'data\\\\test\\\\192.jpg', 'data\\\\test\\\\193.jpg', 'data\\\\test\\\\194.jpg', 'data\\\\test\\\\195.jpg', 'data\\\\test\\\\196.jpg', 'data\\\\test\\\\197.jpg', 'data\\\\test\\\\198.jpg', 'data\\\\test\\\\199.jpg', 'data\\\\test\\\\200.jpg', 'data\\\\test\\\\201.jpg', 'data\\\\test\\\\202.jpg', 'data\\\\test\\\\203.jpg', 'data\\\\test\\\\204.jpg', 'data\\\\test\\\\205.jpg', 'data\\\\test\\\\206.jpg', 'data\\\\test\\\\207.jpg', 'data\\\\test\\\\208.jpg', 'data\\\\test\\\\209.jpg', 'data\\\\test\\\\210.jpg', 'data\\\\test\\\\211.jpg', 'data\\\\test\\\\212.jpg', 'data\\\\test\\\\213.jpg', 'data\\\\test\\\\214.jpg', 'data\\\\test\\\\215.jpg', 'data\\\\test\\\\216.jpg', 'data\\\\test\\\\217.jpg', 'data\\\\test\\\\218.jpg', 'data\\\\test\\\\219.jpg', 'data\\\\test\\\\220.jpg', 'data\\\\test\\\\221.jpg', 'data\\\\test\\\\222.jpg', 'data\\\\test\\\\223.jpg', 'data\\\\test\\\\224.jpg', 'data\\\\test\\\\225.jpg', 'data\\\\test\\\\226.jpg', 'data\\\\test\\\\227.jpg', 'data\\\\test\\\\228.jpg', 'data\\\\test\\\\229.jpg', 'data\\\\test\\\\230.jpg', 'data\\\\test\\\\231.jpg', 'data\\\\test\\\\232.jpg', 'data\\\\test\\\\233.jpg', 'data\\\\test\\\\234.jpg', 'data\\\\test\\\\235.jpg', 'data\\\\test\\\\236.jpg', 'data\\\\test\\\\237.jpg', 'data\\\\test\\\\238.jpg', 'data\\\\test\\\\239.jpg', 'data\\\\test\\\\240.jpg', 'data\\\\test\\\\241.jpg', 'data\\\\test\\\\242.jpg', 'data\\\\test\\\\243.jpg', 'data\\\\test\\\\244.jpg', 'data\\\\test\\\\245.jpg', 'data\\\\test\\\\246.jpg', 'data\\\\test\\\\247.jpg', 'data\\\\test\\\\248.jpg', 'data\\\\test\\\\249.jpg', 'data\\\\test\\\\250.jpg', 'data\\\\test\\\\251.jpg', 'data\\\\test\\\\252.jpg', 'data\\\\test\\\\253.jpg', 'data\\\\test\\\\254.jpg', 'data\\\\test\\\\255.jpg', 'data\\\\test\\\\256.jpg', 'data\\\\test\\\\257.jpg', 'data\\\\test\\\\258.jpg', 'data\\\\test\\\\259.jpg', 'data\\\\test\\\\260.jpg', 'data\\\\test\\\\261.jpg', 'data\\\\test\\\\262.jpg', 'data\\\\test\\\\263.jpg', 'data\\\\test\\\\264.jpg', 'data\\\\test\\\\265.jpg', 'data\\\\test\\\\266.jpg', 'data\\\\test\\\\267.jpg', 'data\\\\test\\\\268.jpg', 'data\\\\test\\\\269.jpg', 'data\\\\test\\\\270.jpg', 'data\\\\test\\\\271.jpg', 'data\\\\test\\\\272.jpg', 'data\\\\test\\\\273.jpg', 'data\\\\test\\\\274.jpg', 'data\\\\test\\\\275.jpg', 'data\\\\test\\\\276.jpg', 'data\\\\test\\\\277.jpg', 'data\\\\test\\\\278.jpg', 'data\\\\test\\\\279.jpg', 'data\\\\test\\\\280.jpg', 'data\\\\test\\\\281.jpg', 'data\\\\test\\\\282.jpg', 'data\\\\test\\\\283.jpg', 'data\\\\test\\\\284.jpg', 'data\\\\test\\\\285.jpg', 'data\\\\test\\\\286.jpg', 'data\\\\test\\\\287.jpg', 'data\\\\test\\\\288.jpg', 'data\\\\test\\\\289.jpg', 'data\\\\test\\\\290.jpg', 'data\\\\test\\\\291.jpg', 'data\\\\test\\\\292.jpg', 'data\\\\test\\\\293.jpg', 'data\\\\test\\\\294.jpg', 'data\\\\test\\\\295.jpg', 'data\\\\test\\\\296.jpg', 'data\\\\test\\\\297.jpg', 'data\\\\test\\\\298.jpg', 'data\\\\test\\\\299.jpg', 'data\\\\test\\\\300.jpg', 'data\\\\test\\\\301.jpg', 'data\\\\test\\\\302.jpg', 'data\\\\test\\\\303.jpg', 'data\\\\test\\\\304.jpg', 'data\\\\test\\\\305.jpg', 'data\\\\test\\\\306.jpg', 'data\\\\test\\\\307.jpg', 'data\\\\test\\\\308.jpg', 'data\\\\test\\\\309.jpg', 'data\\\\test\\\\310.jpg', 'data\\\\test\\\\311.jpg', 'data\\\\test\\\\312.jpg', 'data\\\\test\\\\313.jpg', 'data\\\\test\\\\314.jpg', 'data\\\\test\\\\315.jpg', 'data\\\\test\\\\316.jpg', 'data\\\\test\\\\317.jpg', 'data\\\\test\\\\318.jpg', 'data\\\\test\\\\319.jpg', 'data\\\\test\\\\320.jpg', 'data\\\\test\\\\321.jpg', 'data\\\\test\\\\322.jpg', 'data\\\\test\\\\323.jpg', 'data\\\\test\\\\324.jpg', 'data\\\\test\\\\325.jpg', 'data\\\\test\\\\326.jpg', 'data\\\\test\\\\327.jpg', 'data\\\\test\\\\328.jpg', 'data\\\\test\\\\329.jpg', 'data\\\\test\\\\330.jpg', 'data\\\\test\\\\331.jpg', 'data\\\\test\\\\332.jpg', 'data\\\\test\\\\333.jpg', 'data\\\\test\\\\334.jpg', 'data\\\\test\\\\335.jpg', 'data\\\\test\\\\336.jpg', 'data\\\\test\\\\337.jpg', 'data\\\\test\\\\338.jpg', 'data\\\\test\\\\339.jpg', 'data\\\\test\\\\340.jpg', 'data\\\\test\\\\341.jpg', 'data\\\\test\\\\342.jpg', 'data\\\\test\\\\343.jpg', 'data\\\\test\\\\344.jpg', 'data\\\\test\\\\345.jpg', 'data\\\\test\\\\346.jpg', 'data\\\\test\\\\347.jpg', 'data\\\\test\\\\348.jpg', 'data\\\\test\\\\349.jpg', 'data\\\\test\\\\350.jpg', 'data\\\\test\\\\351.jpg', 'data\\\\test\\\\352.jpg', 'data\\\\test\\\\353.jpg', 'data\\\\test\\\\354.jpg', 'data\\\\test\\\\355.jpg', 'data\\\\test\\\\356.jpg', 'data\\\\test\\\\357.jpg', 'data\\\\test\\\\358.jpg', 'data\\\\test\\\\359.jpg', 'data\\\\test\\\\360.jpg', 'data\\\\test\\\\361.jpg', 'data\\\\test\\\\362.jpg', 'data\\\\test\\\\363.jpg', 'data\\\\test\\\\364.jpg', 'data\\\\test\\\\365.jpg', 'data\\\\test\\\\366.jpg', 'data\\\\test\\\\367.jpg', 'data\\\\test\\\\368.jpg', 'data\\\\test\\\\369.jpg', 'data\\\\test\\\\370.jpg', 'data\\\\test\\\\371.jpg', 'data\\\\test\\\\372.jpg', 'data\\\\test\\\\373.jpg', 'data\\\\test\\\\374.jpg', 'data\\\\test\\\\375.jpg', 'data\\\\test\\\\376.jpg', 'data\\\\test\\\\377.jpg', 'data\\\\test\\\\378.jpg', 'data\\\\test\\\\379.jpg', 'data\\\\test\\\\380.jpg', 'data\\\\test\\\\381.jpg', 'data\\\\test\\\\382.jpg', 'data\\\\test\\\\383.jpg', 'data\\\\test\\\\384.jpg', 'data\\\\test\\\\385.jpg', 'data\\\\test\\\\386.jpg', 'data\\\\test\\\\387.jpg', 'data\\\\test\\\\388.jpg', 'data\\\\test\\\\389.jpg', 'data\\\\test\\\\390.jpg', 'data\\\\test\\\\391.jpg', 'data\\\\test\\\\392.jpg', 'data\\\\test\\\\393.jpg', 'data\\\\test\\\\394.jpg', 'data\\\\test\\\\395.jpg', 'data\\\\test\\\\396.jpg', 'data\\\\test\\\\397.jpg', 'data\\\\test\\\\398.jpg', 'data\\\\test\\\\399.jpg']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\n",
      "    1.000  cats\n",
      "001\n",
      "    1.000  cats\n",
      "002\n",
      "    1.000  cats\n",
      "003\n",
      "    0.000  cats\n",
      "004\n",
      "    1.000  cats\n",
      "005\n",
      "    1.000  cats\n",
      "006\n",
      "    0.000  cats\n",
      "007\n",
      "    1.000  cats\n",
      "008\n",
      "    1.000  cats\n",
      "009\n",
      "    1.000  cats\n",
      "010\n",
      "    0.000  cats\n",
      "011\n",
      "    1.000  cats\n",
      "012\n",
      "    0.000  cats\n",
      "013\n",
      "    1.000  cats\n",
      "014\n",
      "    1.000  cats\n",
      "015\n",
      "    0.000  cats\n",
      "016\n",
      "    0.000  cats\n",
      "017\n",
      "    1.000  cats\n",
      "018\n",
      "    0.002  cats\n",
      "019\n",
      "    1.000  cats\n",
      "020\n",
      "    1.000  cats\n",
      "021\n",
      "    1.000  cats\n",
      "022\n",
      "    1.000  cats\n",
      "023\n",
      "    1.000  cats\n",
      "024\n",
      "    1.000  cats\n",
      "025\n",
      "    0.000  cats\n",
      "026\n",
      "    0.004  cats\n",
      "027\n",
      "    0.000  cats\n",
      "028\n",
      "    0.000  cats\n",
      "029\n",
      "    0.000  cats\n",
      "030\n",
      "    0.000  cats\n",
      "031\n",
      "    0.000  cats\n",
      "032\n",
      "    1.000  cats\n",
      "033\n",
      "    1.000  cats\n",
      "034\n",
      "    1.000  cats\n",
      "035\n",
      "    0.000  cats\n",
      "036\n",
      "    1.000  cats\n",
      "037\n",
      "    1.000  cats\n",
      "038\n",
      "    0.000  cats\n",
      "039\n",
      "    1.000  cats\n",
      "040\n",
      "    0.000  cats\n",
      "041\n",
      "    0.000  cats\n",
      "042\n",
      "    1.000  cats\n",
      "043\n",
      "    1.000  cats\n",
      "044\n",
      "    1.000  cats\n",
      "045\n",
      "    0.000  cats\n",
      "046\n",
      "    1.000  cats\n",
      "047\n",
      "    1.000  cats\n",
      "048\n",
      "    0.730  cats\n",
      "049\n",
      "    1.000  cats\n",
      "050\n",
      "    1.000  cats\n",
      "051\n",
      "    1.000  cats\n",
      "052\n",
      "    0.000  cats\n",
      "053\n",
      "    1.000  cats\n",
      "054\n",
      "    0.000  cats\n",
      "055\n",
      "    0.000  cats\n",
      "056\n",
      "    0.000  cats\n",
      "057\n",
      "    1.000  cats\n",
      "058\n",
      "    1.000  cats\n",
      "059\n",
      "    1.000  cats\n",
      "060\n",
      "    0.000  cats\n",
      "061\n",
      "    1.000  cats\n",
      "062\n",
      "    1.000  cats\n",
      "063\n",
      "    0.149  cats\n",
      "064\n",
      "    1.000  cats\n",
      "065\n",
      "    1.000  cats\n",
      "066\n",
      "    1.000  cats\n",
      "067\n",
      "    1.000  cats\n",
      "068\n",
      "    0.000  cats\n",
      "069\n",
      "    1.000  cats\n",
      "070\n",
      "    0.000  cats\n",
      "071\n",
      "    0.000  cats\n",
      "072\n",
      "    1.000  cats\n",
      "073\n",
      "    1.000  cats\n",
      "074\n",
      "    1.000  cats\n",
      "075\n",
      "    0.990  cats\n",
      "076\n",
      "    1.000  cats\n",
      "077\n",
      "    0.000  cats\n",
      "078\n",
      "    1.000  cats\n",
      "079\n",
      "    0.000  cats\n",
      "080\n",
      "    0.000  cats\n",
      "081\n",
      "    0.001  cats\n",
      "082\n",
      "    1.000  cats\n",
      "083\n",
      "    1.000  cats\n",
      "084\n",
      "    0.010  cats\n",
      "085\n",
      "    0.000  cats\n",
      "086\n",
      "    0.993  cats\n",
      "087\n",
      "    1.000  cats\n",
      "088\n",
      "    1.000  cats\n",
      "089\n",
      "    0.000  cats\n",
      "090\n",
      "    1.000  cats\n",
      "091\n",
      "    0.000  cats\n",
      "092\n",
      "    0.000  cats\n",
      "093\n",
      "    0.000  cats\n",
      "094\n",
      "    1.000  cats\n",
      "095\n",
      "    0.000  cats\n",
      "096\n",
      "    1.000  cats\n",
      "097\n",
      "    1.000  cats\n",
      "098\n",
      "    0.048  cats\n",
      "099\n",
      "    1.000  cats\n",
      "100\n",
      "    0.000  cats\n",
      "101\n",
      "    0.000  cats\n",
      "102\n",
      "    1.000  cats\n",
      "103\n",
      "    0.001  cats\n",
      "104\n",
      "    1.000  cats\n",
      "105\n",
      "    0.000  cats\n",
      "106\n",
      "    0.000  cats\n",
      "107\n",
      "    1.000  cats\n",
      "108\n",
      "    0.000  cats\n",
      "109\n",
      "    0.000  cats\n",
      "110\n",
      "    1.000  cats\n",
      "111\n",
      "    0.003  cats\n",
      "112\n",
      "    1.000  cats\n",
      "113\n",
      "    0.000  cats\n",
      "114\n",
      "    0.000  cats\n",
      "115\n",
      "    0.000  cats\n",
      "116\n",
      "    1.000  cats\n",
      "117\n",
      "    0.000  cats\n",
      "118\n",
      "    0.005  cats\n",
      "119\n",
      "    1.000  cats\n",
      "120\n",
      "    1.000  cats\n",
      "121\n",
      "    0.000  cats\n",
      "122\n",
      "    1.000  cats\n",
      "123\n",
      "    1.000  cats\n",
      "124\n",
      "    0.999  cats\n",
      "125\n",
      "    0.000  cats\n",
      "126\n",
      "    0.000  cats\n",
      "127\n",
      "    0.000  cats\n",
      "128\n",
      "    0.000  cats\n",
      "129\n",
      "    0.999  cats\n",
      "130\n",
      "    0.000  cats\n",
      "131\n",
      "    0.000  cats\n",
      "132\n",
      "    1.000  cats\n",
      "133\n",
      "    1.000  cats\n",
      "134\n",
      "    1.000  cats\n",
      "135\n",
      "    0.000  cats\n",
      "136\n",
      "    1.000  cats\n",
      "137\n",
      "    0.998  cats\n",
      "138\n",
      "    0.000  cats\n",
      "139\n",
      "    0.000  cats\n",
      "140\n",
      "    0.000  cats\n",
      "141\n",
      "    1.000  cats\n",
      "142\n",
      "    1.000  cats\n",
      "143\n",
      "    1.000  cats\n",
      "144\n",
      "    1.000  cats\n",
      "145\n",
      "    0.000  cats\n",
      "146\n",
      "    0.000  cats\n",
      "147\n",
      "    1.000  cats\n",
      "148\n",
      "    0.000  cats\n",
      "149\n",
      "    0.000  cats\n",
      "150\n",
      "    0.992  cats\n",
      "151\n",
      "    1.000  cats\n",
      "152\n",
      "    0.384  cats\n",
      "153\n",
      "    0.000  cats\n",
      "154\n",
      "    0.000  cats\n",
      "155\n",
      "    1.000  cats\n",
      "156\n",
      "    1.000  cats\n",
      "157\n",
      "    0.000  cats\n",
      "158\n",
      "    0.000  cats\n",
      "159\n",
      "    1.000  cats\n",
      "160\n",
      "    0.000  cats\n",
      "161\n",
      "    0.000  cats\n",
      "162\n",
      "    0.000  cats\n",
      "163\n",
      "    0.000  cats\n",
      "164\n",
      "    0.000  cats\n",
      "165\n",
      "    1.000  cats\n",
      "166\n",
      "    1.000  cats\n",
      "167\n",
      "    1.000  cats\n",
      "168\n",
      "    0.000  cats\n",
      "169\n",
      "    1.000  cats\n",
      "170\n",
      "    1.000  cats\n",
      "171\n",
      "    1.000  cats\n",
      "172\n",
      "    1.000  cats\n",
      "173\n",
      "    0.000  cats\n",
      "174\n",
      "    0.000  cats\n",
      "175\n",
      "    1.000  cats\n",
      "176\n",
      "    0.000  cats\n",
      "177\n",
      "    0.000  cats\n",
      "178\n",
      "    1.000  cats\n",
      "179\n",
      "    0.000  cats\n",
      "180\n",
      "    1.000  cats\n",
      "181\n",
      "    1.000  cats\n",
      "182\n",
      "    0.000  cats\n",
      "183\n",
      "    0.000  cats\n",
      "184\n",
      "    0.000  cats\n",
      "185\n",
      "    1.000  cats\n",
      "186\n",
      "    1.000  cats\n",
      "187\n",
      "    0.000  cats\n",
      "188\n",
      "    0.000  cats\n",
      "189\n",
      "    1.000  cats\n",
      "190\n",
      "    0.000  cats\n",
      "191\n",
      "    0.000  cats\n",
      "192\n",
      "    0.000  cats\n",
      "193\n",
      "    1.000  cats\n",
      "194\n",
      "    0.000  cats\n",
      "195\n",
      "    0.000  cats\n",
      "196\n",
      "    0.000  cats\n",
      "197\n",
      "    1.000  cats\n",
      "198\n",
      "    1.000  cats\n",
      "199\n",
      "    0.000  cats\n",
      "200\n",
      "    0.995  cats\n",
      "201\n",
      "    0.000  cats\n",
      "202\n",
      "    0.000  cats\n",
      "203\n",
      "    0.000  cats\n",
      "204\n",
      "    0.000  cats\n",
      "205\n",
      "    0.863  cats\n",
      "206\n",
      "    1.000  cats\n",
      "207\n",
      "    0.000  cats\n",
      "208\n",
      "    0.000  cats\n",
      "209\n",
      "    1.000  cats\n",
      "210\n",
      "    1.000  cats\n",
      "211\n",
      "    0.000  cats\n",
      "212\n",
      "    0.000  cats\n",
      "213\n",
      "    0.000  cats\n",
      "214\n",
      "    0.000  cats\n",
      "215\n",
      "    0.000  cats\n",
      "216\n",
      "    0.000  cats\n",
      "217\n",
      "    1.000  cats\n",
      "218\n",
      "    0.000  cats\n",
      "219\n",
      "    1.000  cats\n",
      "220\n",
      "    0.000  cats\n",
      "221\n",
      "    0.000  cats\n",
      "222\n",
      "    0.000  cats\n",
      "223\n",
      "    1.000  cats\n",
      "224\n",
      "    1.000  cats\n",
      "225\n",
      "    0.000  cats\n",
      "226\n",
      "    1.000  cats\n",
      "227\n",
      "    1.000  cats\n",
      "228\n",
      "    1.000  cats\n",
      "229\n",
      "    0.000  cats\n",
      "230\n",
      "    0.000  cats\n",
      "231\n",
      "    1.000  cats\n",
      "232\n",
      "    1.000  cats\n",
      "233\n",
      "    0.000  cats\n",
      "234\n",
      "    0.000  cats\n",
      "235\n",
      "    1.000  cats\n",
      "236\n",
      "    1.000  cats\n",
      "237\n",
      "    0.000  cats\n",
      "238\n",
      "    0.915  cats\n",
      "239\n",
      "    0.000  cats\n",
      "240\n",
      "    1.000  cats\n",
      "241\n",
      "    0.943  cats\n",
      "242\n",
      "    0.000  cats\n",
      "243\n",
      "    0.000  cats\n",
      "244\n",
      "    1.000  cats\n",
      "245\n",
      "    1.000  cats\n",
      "246\n",
      "    1.000  cats\n",
      "247\n",
      "    0.000  cats\n",
      "248\n",
      "    0.000  cats\n",
      "249\n",
      "    0.001  cats\n",
      "250\n",
      "    0.000  cats\n",
      "251\n",
      "    0.015  cats\n",
      "252\n",
      "    0.999  cats\n",
      "253\n",
      "    0.000  cats\n",
      "254\n",
      "    0.000  cats\n",
      "255\n",
      "    0.009  cats\n",
      "256\n",
      "    1.000  cats\n",
      "257\n",
      "    1.000  cats\n",
      "258\n",
      "    1.000  cats\n",
      "259\n",
      "    0.994  cats\n",
      "260\n",
      "    1.000  cats\n",
      "261\n",
      "    1.000  cats\n",
      "262\n",
      "    1.000  cats\n",
      "263\n",
      "    0.000  cats\n",
      "264\n",
      "    0.000  cats\n",
      "265\n",
      "    0.000  cats\n",
      "266\n",
      "    0.000  cats\n",
      "267\n",
      "    0.000  cats\n",
      "268\n",
      "    1.000  cats\n",
      "269\n",
      "    0.000  cats\n",
      "270\n",
      "    1.000  cats\n",
      "271\n",
      "    0.085  cats\n",
      "272\n",
      "    0.000  cats\n",
      "273\n",
      "    0.000  cats\n",
      "274\n",
      "    1.000  cats\n",
      "275\n",
      "    0.000  cats\n",
      "276\n",
      "    0.000  cats\n",
      "277\n",
      "    0.000  cats\n",
      "278\n",
      "    0.000  cats\n",
      "279\n",
      "    0.000  cats\n",
      "280\n",
      "    1.000  cats\n",
      "281\n",
      "    0.000  cats\n",
      "282\n",
      "    0.000  cats\n",
      "283\n",
      "    0.000  cats\n",
      "284\n",
      "    0.000  cats\n",
      "285\n",
      "    1.000  cats\n",
      "286\n",
      "    1.000  cats\n",
      "287\n",
      "    1.000  cats\n",
      "288\n",
      "    1.000  cats\n",
      "289\n",
      "    0.000  cats\n",
      "290\n",
      "    1.000  cats\n",
      "291\n",
      "    1.000  cats\n",
      "292\n",
      "    0.999  cats\n",
      "293\n",
      "    1.000  cats\n",
      "294\n",
      "    1.000  cats\n",
      "295\n",
      "    1.000  cats\n",
      "296\n",
      "    0.997  cats\n",
      "297\n",
      "    0.046  cats\n",
      "298\n",
      "    1.000  cats\n",
      "299\n",
      "    1.000  cats\n",
      "300\n",
      "    1.000  cats\n",
      "301\n",
      "    0.000  cats\n",
      "302\n",
      "    0.000  cats\n",
      "303\n",
      "    0.000  cats\n",
      "304\n",
      "    0.000  cats\n",
      "305\n",
      "    0.000  cats\n",
      "306\n",
      "    0.000  cats\n",
      "307\n",
      "    0.000  cats\n",
      "308\n",
      "    1.000  cats\n",
      "309\n",
      "    1.000  cats\n",
      "310\n",
      "    1.000  cats\n",
      "311\n",
      "    0.000  cats\n",
      "312\n",
      "    0.000  cats\n",
      "313\n",
      "    0.000  cats\n",
      "314\n",
      "    0.002  cats\n",
      "315\n",
      "    0.000  cats\n",
      "316\n",
      "    0.000  cats\n",
      "317\n",
      "    0.999  cats\n",
      "318\n",
      "    1.000  cats\n",
      "319\n",
      "    1.000  cats\n",
      "320\n",
      "    1.000  cats\n",
      "321\n",
      "    1.000  cats\n",
      "322\n",
      "    0.000  cats\n",
      "323\n",
      "    0.000  cats\n",
      "324\n",
      "    0.000  cats\n",
      "325\n",
      "    0.000  cats\n",
      "326\n",
      "    1.000  cats\n",
      "327\n",
      "    1.000  cats\n",
      "328\n",
      "    1.000  cats\n",
      "329\n",
      "    1.000  cats\n",
      "330\n",
      "    0.000  cats\n",
      "331\n",
      "    0.000  cats\n",
      "332\n",
      "    1.000  cats\n",
      "333\n",
      "    0.000  cats\n",
      "334\n",
      "    0.000  cats\n",
      "335\n",
      "    1.000  cats\n",
      "336\n",
      "    1.000  cats\n",
      "337\n",
      "    1.000  cats\n",
      "338\n",
      "    0.000  cats\n",
      "339\n",
      "    0.094  cats\n",
      "340\n",
      "    0.000  cats\n",
      "341\n",
      "    1.000  cats\n",
      "342\n",
      "    1.000  cats\n",
      "343\n",
      "    0.000  cats\n",
      "344\n",
      "    0.000  cats\n",
      "345\n",
      "    0.000  cats\n",
      "346\n",
      "    0.000  cats\n",
      "347\n",
      "    0.000  cats\n",
      "348\n",
      "    1.000  cats\n",
      "349\n",
      "    1.000  cats\n",
      "350\n",
      "    0.000  cats\n",
      "351\n",
      "    1.000  cats\n",
      "352\n",
      "    0.000  cats\n",
      "353\n",
      "    1.000  cats\n",
      "354\n",
      "    0.000  cats\n",
      "355\n",
      "    1.000  cats\n",
      "356\n",
      "    1.000  cats\n",
      "357\n",
      "    1.000  cats\n",
      "358\n",
      "    1.000  cats\n",
      "359\n",
      "    1.000  cats\n",
      "360\n",
      "    0.000  cats\n",
      "361\n",
      "    1.000  cats\n",
      "362\n",
      "    1.000  cats\n",
      "363\n",
      "    0.000  cats\n",
      "364\n",
      "    1.000  cats\n",
      "365\n",
      "    1.000  cats\n",
      "366\n",
      "    0.000  cats\n",
      "367\n",
      "    0.000  cats\n",
      "368\n",
      "    1.000  cats\n",
      "369\n",
      "    0.000  cats\n",
      "370\n",
      "    1.000  cats\n",
      "371\n",
      "    0.000  cats\n",
      "372\n",
      "    1.000  cats\n",
      "373\n",
      "    0.999  cats\n",
      "374\n",
      "    1.000  cats\n",
      "375\n",
      "    0.008  cats\n",
      "376\n",
      "    1.000  cats\n",
      "377\n",
      "    1.000  cats\n",
      "378\n",
      "    1.000  cats\n",
      "379\n",
      "    0.187  cats\n",
      "380\n",
      "    0.000  cats\n",
      "381\n",
      "    1.000  cats\n",
      "382\n",
      "    0.000  cats\n",
      "383\n",
      "    1.000  cats\n",
      "384\n",
      "    1.000  cats\n",
      "385\n",
      "    0.000  cats\n",
      "386\n",
      "    0.000  cats\n",
      "387\n",
      "    0.000  cats\n",
      "388\n",
      "    0.000  cats\n",
      "389\n",
      "    0.000  cats\n",
      "390\n",
      "    1.000  cats\n",
      "391\n",
      "    0.000  cats\n",
      "392\n",
      "    0.000  cats\n",
      "393\n",
      "    1.000  cats\n",
      "394\n",
      "    0.000  cats\n",
      "395\n",
      "    0.000  cats\n",
      "396\n",
      "    1.000  cats\n",
      "397\n",
      "    1.000  cats\n",
      "398\n",
      "    1.000  cats\n",
      "399\n",
      "    1.000  cats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003</td>\n",
       "      <td>0.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004</td>\n",
       "      <td>0.9999763965606689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>005</td>\n",
       "      <td>0.9999997615814209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>006</td>\n",
       "      <td>0.0000000821091675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>007</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>008</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>009</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>010</td>\n",
       "      <td>0.0000030396595321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>011</td>\n",
       "      <td>0.9999983310699463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>012</td>\n",
       "      <td>0.0000000000001476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>013</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>014</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>015</td>\n",
       "      <td>0.0000152356396939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>016</td>\n",
       "      <td>0.0000000001015971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>017</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>018</td>\n",
       "      <td>0.0020704169292003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>019</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>020</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>021</td>\n",
       "      <td>0.9997094273567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>022</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>023</td>\n",
       "      <td>0.9999997615814209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>024</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>025</td>\n",
       "      <td>0.0000000000000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>026</td>\n",
       "      <td>0.0040818676352501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>027</td>\n",
       "      <td>0.0000000000007479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>028</td>\n",
       "      <td>0.0000000000306508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>029</td>\n",
       "      <td>0.0000000051836175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>370</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>371</td>\n",
       "      <td>0.0000003894211318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>372</td>\n",
       "      <td>0.9999992847442627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>373</td>\n",
       "      <td>0.9993464350700378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>374</td>\n",
       "      <td>0.9999998807907104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>0.0078780744224787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>376</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>377</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>378</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>379</td>\n",
       "      <td>0.1871897280216217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>380</td>\n",
       "      <td>0.0000000001012671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>381</td>\n",
       "      <td>0.9995587468147278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>0.0000000000000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>383</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>384</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>385</td>\n",
       "      <td>0.0000000000693411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>386</td>\n",
       "      <td>0.0000000000037054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>387</td>\n",
       "      <td>0.0000000000000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>388</td>\n",
       "      <td>0.0000014839762343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>389</td>\n",
       "      <td>0.0000000048335433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>390</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>391</td>\n",
       "      <td>0.0000000000004547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>0.0000000000000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>393</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>394</td>\n",
       "      <td>0.0000000074248216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>0.0000000008819072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>397</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID           Predicted\n",
       "0    000  1.0000000000000000\n",
       "1    001  1.0000000000000000\n",
       "2    002  1.0000000000000000\n",
       "3    003  0.0000000000000000\n",
       "4    004  0.9999763965606689\n",
       "5    005  0.9999997615814209\n",
       "6    006  0.0000000821091675\n",
       "7    007  1.0000000000000000\n",
       "8    008  1.0000000000000000\n",
       "9    009  1.0000000000000000\n",
       "10   010  0.0000030396595321\n",
       "11   011  0.9999983310699463\n",
       "12   012  0.0000000000001476\n",
       "13   013  1.0000000000000000\n",
       "14   014  1.0000000000000000\n",
       "15   015  0.0000152356396939\n",
       "16   016  0.0000000001015971\n",
       "17   017  1.0000000000000000\n",
       "18   018  0.0020704169292003\n",
       "19   019  1.0000000000000000\n",
       "20   020  1.0000000000000000\n",
       "21   021  0.9997094273567200\n",
       "22   022  1.0000000000000000\n",
       "23   023  0.9999997615814209\n",
       "24   024  1.0000000000000000\n",
       "25   025  0.0000000000000236\n",
       "26   026  0.0040818676352501\n",
       "27   027  0.0000000000007479\n",
       "28   028  0.0000000000306508\n",
       "29   029  0.0000000051836175\n",
       "..   ...                 ...\n",
       "370  370  1.0000000000000000\n",
       "371  371  0.0000003894211318\n",
       "372  372  0.9999992847442627\n",
       "373  373  0.9993464350700378\n",
       "374  374  0.9999998807907104\n",
       "375  375  0.0078780744224787\n",
       "376  376  1.0000000000000000\n",
       "377  377  1.0000000000000000\n",
       "378  378  1.0000000000000000\n",
       "379  379  0.1871897280216217\n",
       "380  380  0.0000000001012671\n",
       "381  381  0.9995587468147278\n",
       "382  382  0.0000000000000064\n",
       "383  383  1.0000000000000000\n",
       "384  384  1.0000000000000000\n",
       "385  385  0.0000000000693411\n",
       "386  386  0.0000000000037054\n",
       "387  387  0.0000000000000105\n",
       "388  388  0.0000014839762343\n",
       "389  389  0.0000000048335433\n",
       "390  390  1.0000000000000000\n",
       "391  391  0.0000000000004547\n",
       "392  392  0.0000000000000268\n",
       "393  393  1.0000000000000000\n",
       "394  394  0.0000000074248216\n",
       "395  395  0.0000000008819072\n",
       "396  396  1.0000000000000000\n",
       "397  397  1.0000000000000000\n",
       "398  398  1.0000000000000000\n",
       "399  399  1.0000000000000000\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "\n",
    "import os\n",
    "\n",
    "path = 'data\\\\test'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.jpg' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "print (files)\n",
    "\n",
    "# \n",
    "net = load_model(file_path)\n",
    "\n",
    "cls_list = ['cats', 'dogs']\n",
    "cols=['ID', 'Predicted']\n",
    "df=pd.DataFrame(columns=cols)\n",
    "# \n",
    "for f in files:\n",
    "    img = image.load_img(f, target_size=(272, 272))\n",
    "    if img is None:\n",
    "        continue\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    pred = net.predict(x)[0]\n",
    "    top_inds = pred.argsort()[::-1][:5]\n",
    "    print(f.replace('data\\\\test\\\\','').replace('.jpg',''))\n",
    "    print('    {:.3f}  {}'.format(pred[0], cls_list[0]))\n",
    "    df=df.append({'ID':f.replace('data\\\\test\\\\','').replace('.jpg',''), 'Predicted':'{:.16f}'.format(pred[0])},ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002</td>\n",
       "      <td>1.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003</td>\n",
       "      <td>0.0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004</td>\n",
       "      <td>0.9999763965606689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID           Predicted\n",
       "0  000  1.0000000000000000\n",
       "1  001  1.0000000000000000\n",
       "2  002  1.0000000000000000\n",
       "3  003  0.0000000000000000\n",
       "4  004  0.9999763965606689"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission08181200.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
